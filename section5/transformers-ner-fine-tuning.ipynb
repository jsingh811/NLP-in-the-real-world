{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7f4bbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.22.2)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-macosx_10_11_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 6.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.3.1)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "\u001b[K     |████████████████████████████████| 182 kB 17.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: requests in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb049015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=389e97782c6e9ba19a374af49bd00d6b9423ef0ca4c3e6143aca74c33093559d\n",
      "  Stored in directory: /Users/jsingh/Library/Caches/pip/wheels/04/5f/3e/46cc37c5d698415694d83f607f833f83f0149e49b3af9d0f38\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "! python -m pip install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c01cf42",
   "metadata": {},
   "source": [
    "# downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d41c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-05 15:20:06--  http://noisy-text.github.io/2017/files/wnut17train.conll\n",
      "Resolving noisy-text.github.io (noisy-text.github.io)... 185.199.111.153, 185.199.110.153, 185.199.108.153, ...\n",
      "Connecting to noisy-text.github.io (noisy-text.github.io)|185.199.111.153|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 493781 (482K) [application/octet-stream]\n",
      "Saving to: ‘wnut17train.conll’\n",
      "\n",
      "wnut17train.conll   100%[===================>] 482.21K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-12-05 15:20:07 (3.22 MB/s) - ‘wnut17train.conll’ saved [493781/493781]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget http://noisy-text.github.io/2017/files/wnut17train.conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa2a3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def split_into_tokens(raw_text):\n",
    "    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    for doc in raw_docs:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in doc.split('\\n'):\n",
    "            row = line.split('\\t')\n",
    "            if len(row) == 1:\n",
    "                token = row[0]\n",
    "                tag = None\n",
    "            else:\n",
    "                token, tag = line.split('\\t')\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        token_docs.append(tokens)\n",
    "        tag_docs.append(tags)\n",
    "\n",
    "    return token_docs, tag_docs\n",
    "\n",
    "def read_wnut(file_path):\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    raw_text = file_path.read_text().strip()\n",
    "    token_docs, tag_docs = split_into_tokens(raw_text)\n",
    "\n",
    "    return token_docs, tag_docs\n",
    "\n",
    "texts, tags = read_wnut('wnut17train.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2898d245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'two', 'weeks', '.', 'Empire', 'State', 'Building']\n",
      "['O', 'O', 'O', 'O', 'B-location', 'I-location', 'I-location']\n"
     ]
    }
   ],
   "source": [
    "print(texts[0][10:17], tags[0][10:17], sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c1598a",
   "metadata": {},
   "source": [
    " O indicates the token does not correspond to any entity\n",
    " location is an entity.\n",
    " \n",
    " B- indicates the beginning of an entity, and I- indicates consecutive positions of the same entity.\n",
    " Thus, \"Empire\", \"State\", \"Building\" has tokens \"B-location\", \"I-location\", \"I-location\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "856f67d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our data into training and validation set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6612e6",
   "metadata": {},
   "source": [
    "To encode the tokens, we will use a pre-trained DistilBert tokenizer.\n",
    "We can tell the tokenizer that we have ready-split tokens rather than full sentence strings by passing is_split_into_words=True\n",
    "We’ll pass padding=True and truncation=True to pad the sequences to be the same length. \n",
    "We can tell the model to return information about the tokens that are split by the wordpiece tokenization process.\n",
    "\n",
    "WordPiece Tokenization is the process by which single words are split into multiple tokens such that each token is likely to be in the vocabulary. Some words may not be in the vocabulary of a model. Thus the model splits the word into sub-words/tokens. Since we have only one tag per token, if the tokenizer splits a token into multiple sub-tokens, then we will end up with a mismatch between our tokens and our labels. To resolve this, we will train on the tag labels for the first subtoken of a split token. We can do this by setting the labels we wish to ignore to -100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c172d957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f519e0da63a6415992c7330c791be7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a37ce3b13c4f71bfe709a5401c210e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bece1bd92d4dba9db7e2767ebfa8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017c60fee57f415bb5ead9c18ea9ccc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "train_encodings = tokenizer(\n",
    "    train_texts,\n",
    "    is_split_into_words=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    val_texts, \n",
    "    is_split_into_words=True, \n",
    "    return_offsets_mapping=True, \n",
    "    padding=True, \n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aea25443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "unique_tags = set(tag for doc in tags for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "\n",
    "def encode_tags(tags, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "train_labels = encode_tags(train_tags, train_encodings)\n",
    "val_labels = encode_tags(val_tags, val_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9118e7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 13 entity tags in the data: dict_keys(['I-corporation', 'I-product', 'I-person', 'I-group', 'B-location', 'O', 'I-location', 'B-creative-work', 'B-group', 'I-creative-work', 'B-person', 'B-product', 'B-corporation'])\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are total {len(tag2id.keys())} entity tags in the data: {tag2id.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2497a7e",
   "metadata": {},
   "source": [
    "Next, we will create a dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "786c9a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 15:23:02.500674: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
    "val_encodings.pop(\"offset_mapping\")\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3847653",
   "metadata": {},
   "source": [
    "Now we can load in a token classification model and specify the number of labels. Then, our model is ready for fine-tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfe2f977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d000586138e426896548d50a40a6d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/354M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 15:24:07.866579: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForTokenClassification: ['vocab_layer_norm', 'vocab_projector', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['dropout_19', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertForTokenClassification\n",
    "model = TFDistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(unique_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d32c96",
   "metadata": {},
   "source": [
    "# Fine-tuning our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45c44cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsingh/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:377: FutureWarning: The old compute_loss method is deprecated as it conflicts with the Keras compute_loss method added in TF 2.8. If you want the original HF compute_loss, please call hf_compute_loss() instead. From TF versions >= 2.8, or Transformers versions >= 5, calling compute_loss() will get the Keras method instead.\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/170 [==============================] - 310s 2s/step - loss: 0.2828\n",
      "Epoch 2/3\n",
      "170/170 [==============================] - 303s 2s/step - loss: 0.1332\n",
      "Epoch 3/3\n",
      "170/170 [==============================] - 304s 2s/step - loss: 0.0681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fde979f8c70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss) # you can also use any keras loss fn\n",
    "model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa3bdda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-cased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\",\n",
       "    \"6\": \"LABEL_6\",\n",
       "    \"7\": \"LABEL_7\",\n",
       "    \"8\": \"LABEL_8\",\n",
       "    \"9\": \"LABEL_9\",\n",
       "    \"10\": \"LABEL_10\",\n",
       "    \"11\": \"LABEL_11\",\n",
       "    \"12\": \"LABEL_12\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_10\": 10,\n",
       "    \"LABEL_11\": 11,\n",
       "    \"LABEL_12\": 12,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5,\n",
       "    \"LABEL_6\": 6,\n",
       "    \"LABEL_7\": 7,\n",
       "    \"LABEL_8\": 8,\n",
       "    \"LABEL_9\": 9\n",
       "  },\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.25.1\",\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01d0e259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'LABEL_10', 'score': 0.95119816, 'word': 'Tommy', 'start': 0, 'end': 5}, {'entity_group': 'LABEL_5', 'score': 0.82647365, 'word': 'bought my', 'start': 6, 'end': 15}, {'entity_group': 'LABEL_11', 'score': 0.3976339, 'word': 'Arm', 'start': 16, 'end': 19}, {'entity_group': 'LABEL_1', 'score': 0.5137349, 'word': '##ani', 'start': 19, 'end': 22}, {'entity_group': 'LABEL_5', 'score': 0.97369736, 'word': 'shoes by the', 'start': 23, 'end': 35}, {'entity_group': 'LABEL_4', 'score': 0.37326077, 'word': 'New', 'start': 36, 'end': 39}, {'entity_group': 'LABEL_6', 'score': 0.6780467, 'word': 'Town Mall', 'start': 40, 'end': 49}, {'entity_group': 'LABEL_5', 'score': 0.844704, 'word': 'in Paris.', 'start': 50, 'end': 59}, {'entity_group': 'LABEL_10', 'score': 0.97740185, 'word': 'Ella', 'start': 65, 'end': 69}, {'entity_group': 'LABEL_2', 'score': 0.97186667, 'word': 'Parker', 'start': 70, 'end': 76}, {'entity_group': 'LABEL_5', 'score': 0.9917011, 'word': 'purchased a', 'start': 77, 'end': 88}, {'entity_group': 'LABEL_11', 'score': 0.39736107, 'word': 'Samsung', 'start': 89, 'end': 96}, {'entity_group': 'LABEL_1', 'score': 0.65990174, 'word': 'Galaxy', 'start': 97, 'end': 103}, {'entity_group': 'LABEL_5', 'score': 0.77520126, 'word': 's21 + from', 'start': 104, 'end': 113}, {'entity_group': 'LABEL_4', 'score': 0.41146958, 'word': 'El', 'start': 114, 'end': 116}, {'entity_group': 'LABEL_0', 'score': 0.23474006, 'word': '##ante', 'start': 116, 'end': 120}, {'entity_group': 'LABEL_5', 'score': 0.87043536, 'word': 'mall.', 'start': 121, 'end': 126}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "                         \n",
    "custom_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\") \n",
    "output = custom_ner(\"\"\"Tommy bought my Armani shoes by the New Town Mall in Paris.\n",
    "     Ella Parker purchased a Samsung Galaxy s21+ from Elante mall.\"\"\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f7802a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'B-person',\n",
       "  'score': 0.95119816,\n",
       "  'word': 'Tommy',\n",
       "  'start': 0,\n",
       "  'end': 5},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.82647365,\n",
       "  'word': 'bought my',\n",
       "  'start': 6,\n",
       "  'end': 15},\n",
       " {'entity_group': 'B-product',\n",
       "  'score': 0.3976339,\n",
       "  'word': 'Arm',\n",
       "  'start': 16,\n",
       "  'end': 19},\n",
       " {'entity_group': 'I-product',\n",
       "  'score': 0.5137349,\n",
       "  'word': '##ani',\n",
       "  'start': 19,\n",
       "  'end': 22},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.97369736,\n",
       "  'word': 'shoes by the',\n",
       "  'start': 23,\n",
       "  'end': 35},\n",
       " {'entity_group': 'B-location',\n",
       "  'score': 0.37326077,\n",
       "  'word': 'New',\n",
       "  'start': 36,\n",
       "  'end': 39},\n",
       " {'entity_group': 'I-location',\n",
       "  'score': 0.6780467,\n",
       "  'word': 'Town Mall',\n",
       "  'start': 40,\n",
       "  'end': 49},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.844704,\n",
       "  'word': 'in Paris.',\n",
       "  'start': 50,\n",
       "  'end': 59},\n",
       " {'entity_group': 'B-person',\n",
       "  'score': 0.97740185,\n",
       "  'word': 'Ella',\n",
       "  'start': 65,\n",
       "  'end': 69},\n",
       " {'entity_group': 'I-person',\n",
       "  'score': 0.97186667,\n",
       "  'word': 'Parker',\n",
       "  'start': 70,\n",
       "  'end': 76},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.9917011,\n",
       "  'word': 'purchased a',\n",
       "  'start': 77,\n",
       "  'end': 88},\n",
       " {'entity_group': 'B-product',\n",
       "  'score': 0.39736107,\n",
       "  'word': 'Samsung',\n",
       "  'start': 89,\n",
       "  'end': 96},\n",
       " {'entity_group': 'I-product',\n",
       "  'score': 0.65990174,\n",
       "  'word': 'Galaxy',\n",
       "  'start': 97,\n",
       "  'end': 103},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.77520126,\n",
       "  'word': 's21 + from',\n",
       "  'start': 104,\n",
       "  'end': 113},\n",
       " {'entity_group': 'B-location',\n",
       "  'score': 0.41146958,\n",
       "  'word': 'El',\n",
       "  'start': 114,\n",
       "  'end': 116},\n",
       " {'entity_group': 'I-corporation',\n",
       "  'score': 0.23474006,\n",
       "  'word': '##ante',\n",
       "  'start': 116,\n",
       "  'end': 120},\n",
       " {'entity_group': 'O',\n",
       "  'score': 0.87043536,\n",
       "  'word': 'mall.',\n",
       "  'start': 121,\n",
       "  'end': 126}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_id_to_name(label_result):\n",
    "    output_result = {}\n",
    "    label = label_result[\"entity_group\"]\n",
    "    output_result[\"entity_group\"] = id2tag.get(\n",
    "        int(label.split(\"_\")[1]), \n",
    "        label\n",
    "    )\n",
    "    for key in label_result:\n",
    "        if key != \"entity_group\":\n",
    "            output_result[key] = label_result[key]\n",
    "    return output_result\n",
    "\n",
    "new_output = [convert_id_to_name(i) for i in output]\n",
    "new_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef74c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
