{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b5326c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy==3.4.3\n",
      "  Downloading spacy-3.4.3-cp39-cp39-macosx_10_9_x86_64.whl (6.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.10.0-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp39-cp39-macosx_10_9_x86_64.whl (18 kB)\n",
      "Collecting requests<3.0.0,>=2.13.0\n",
      "  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp39-cp39-macosx_10_9_x86_64.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.9/107.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.5-cp39-cp39-macosx_10_9_x86_64.whl (491 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.10\n",
      "  Downloading spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.2-cp39-cp39-macosx_10_9_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/jyotikasingh/opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy==3.4.3) (3.1.2)\n",
      "Collecting numpy>=1.15.0\n",
      "  Downloading numpy-1.23.5-cp39-cp39-macosx_10_9_x86_64.whl (18.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/jyotikasingh/opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy==3.4.3) (65.5.0)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.5-cp39-cp39-macosx_10_9_x86_64.whl (768 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768.7/768.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/jyotikasingh/opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy==3.4.3) (21.3)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp39-cp39-macosx_10_9_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/jyotikasingh/opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from packaging>=20.0->spacy==3.4.3) (3.0.9)\n",
      "Collecting smart-open<6.0.0,>=5.2.1\n",
      "  Using cached smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Collecting typing-extensions>=4.1.0\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /Users/jyotikasingh/opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.3) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jyotikasingh/opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.3) (3.4)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.3-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp39-cp39-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting click<9.0.0,>=7.1.1\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jyotikasingh/opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jinja2->spacy==3.4.3) (2.1.1)\n",
      "Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, spacy-loggers, spacy-legacy, smart-open, numpy, murmurhash, langcodes, click, charset-normalizer, catalogue, typer, srsly, requests, pydantic, preshed, blis, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 numpy-1.23.5 pathy-0.10.0 preshed-3.0.8 pydantic-1.10.2 requests-2.28.1 smart-open-5.2.1 spacy-3.4.3 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.5 thinc-8.1.5 tqdm-4.64.1 typer-0.7.0 typing-extensions-4.4.0 urllib3-1.26.13 wasabi-0.10.1\n",
      "Collecting textblob==0.17.1\n",
      "  Using cached textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Collecting nltk>=3.1\n",
      "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.10.31-cp39-cp39-macosx_10_9_x86_64.whl (293 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.9/293.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /Users/jyotikasingh/opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nltk>=3.1->textblob==0.17.1) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /Users/jyotikasingh/opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nltk>=3.1->textblob==0.17.1) (4.64.1)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, joblib, nltk, textblob\n",
      "Successfully installed joblib-1.2.0 nltk-3.7 regex-2022.10.31 textblob-0.17.1\n",
      "Collecting nltk==3.6.5\n",
      "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/jyotikasingh/opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nltk==3.6.5) (4.64.1)\n",
      "Requirement already satisfied: joblib in /Users/jyotikasingh/opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nltk==3.6.5) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/jyotikasingh/opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nltk==3.6.5) (2022.10.31)\n",
      "Requirement already satisfied: click in /Users/jyotikasingh/opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nltk==3.6.5) (8.1.3)\n",
      "Installing collected packages: nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.7\n",
      "    Uninstalling nltk-3.7:\n",
      "      Successfully uninstalled nltk-3.7\n",
      "Successfully installed nltk-3.6.5\n",
      "Collecting pyenchant\n",
      "  Using cached pyenchant-3.2.2-py3-none-any.whl (55 kB)\n",
      "Installing collected packages: pyenchant\n",
      "Successfully installed pyenchant-3.2.2\n",
      "Collecting pyspellchecker==0.7.0\n",
      "  Using cached pyspellchecker-0.7.0-py3-none-any.whl (2.5 MB)\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.7.0\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy==3.4.3\n",
    "! pip install textblob==0.17.1\n",
    "! pip install nltk==3.6.5\n",
    "! pip install pyenchant==3.2.2\n",
    "! pip install pyspellchecker==0.7.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3502769f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-31 21:01:33.864456: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.6)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.26.0)\n",
      "Requirement already satisfied: setuptools in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (58.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.22.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
      "Requirement already satisfied: jinja2 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.12)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.62.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2021.10.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/jsingh/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 3.5.0\n",
      "    Uninstalling en-core-web-sm-3.5.0:\n",
      "      Successfully uninstalled en-core-web-sm-3.5.0\n",
      "Successfully installed en-core-web-sm-3.4.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def8c5c",
   "metadata": {},
   "source": [
    "# Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2788a386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi!.\n",
      "I like NLP.\n",
      "Do you??\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "doc = nlp(u\"Hi!. I like NLP. Do you??\")\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e50f648c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I like it.', 'Did you like it too?']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(\"I like it. Did you like it too?\")\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d894def3",
   "metadata": {},
   "source": [
    "# Word tokentization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b539e578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'I', 'like', 'NLP', 'Do', 'you', 'Do', 'you', 'live', 'in', 'the', 'U.K']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"Hi! I like NLP. Do you?? Do you live in the U.K.?\"\n",
    "tokens = TextBlob(text).words\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac1b388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', '!', 'I', 'like', 'NLP', '.', 'Do', 'you', '?', '?', 'Do', 'you', 'live', 'in', 'the', 'U.K.', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "text = \"Hi! I like NLP. Do you?? Do you live in the U.K.?\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e20c9689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hi, !, I, like, NLP, ., Do, you, ?, ?, Do, you, live, in, the, U.K., ?]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# spaCy offers many pre-trained models that you can choose from\n",
    "\n",
    "text = \"Hi! I like NLP. Do you?? Do you live in the U.K.?\"\n",
    "doc = nlp(text)\n",
    "print([token for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a02223",
   "metadata": {},
   "source": [
    "# Part-of-speech (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d008841c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Can', 'MD'), ('you', 'PRP'), ('please', 'VB'), ('buy', 'VB'), ('me', 'PRP'), ('an', 'DT'), ('Arizona', 'NNP'), ('Ice', 'NNP'), ('Tea', 'NNP'), ('?', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('$', '$'), ('0.57', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "tokens = word_tokenize(\n",
    "    \"Can you please buy me an Arizona Ice Tea? It's $0.57.\"\n",
    ")\n",
    "pos = pos_tag(tokens)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628c06c6",
   "metadata": {},
   "source": [
    "# N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48329ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['natural', 'language']), WordList(['language', 'processing'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"natural language processing\"\n",
    "\n",
    "TextBlob(text).ngrams(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2522a493",
   "metadata": {},
   "source": [
    "# Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05703257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi I like NLP do you\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hi. I like NLP, do you?\"\n",
    "\n",
    "# .sub substitutes all matches with empty string below\n",
    "punc_cleaned = re.sub(r'[^\\w\\s]', '', text)\n",
    "print(punc_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d80116a",
   "metadata": {},
   "source": [
    "# URL removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53b8bb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Check it out on  or  for more information. Reach out to abc@xyz.com for inquiries.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"\n",
    "    Check it out on https://google.com or www.google.com for more information. Reach out to abc@xyz.com for inquiries.\n",
    "\"\"\"\n",
    "\n",
    "url_cleaned = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "print(url_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfce9291",
   "metadata": {},
   "source": [
    "# Emoji removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa69e8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does  emoji mean?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"What does 😲 emoji mean?\"\n",
    "emoji_cleaned = re.sub(\n",
    "    r'[\\U00010000-\\U0010ffff]', '' , text, flags=re.UNICODE\n",
    ")\n",
    "print(emoji_cleaned)\n",
    "# >> 'What does  emoji mean?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d46e0",
   "metadata": {},
   "source": [
    "# Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d20fed9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mispell -> misspell\n",
      "craazy -> crazy\n",
      "craaaazy -> None\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# List the words that might be misspelled\n",
    "misspelled = spell.unknown(\n",
    "    ['mispell', 'craazy', 'craaaazy']\n",
    ")\n",
    "\n",
    "for word in misspelled:\n",
    "    # Get the one `most likely` answer\n",
    "    print(f\"{word} -> {spell.correction(word)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fec6db87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you sure about your decision?\n",
      "Are you suture about your decision?\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "data = \"Are yu suuree about your decisiion?\"\n",
    "output = TextBlob(data).correct()\n",
    "print(output)\n",
    "\n",
    "data = \"Are yu suuuree about your decisiion?\"\n",
    "output = TextBlob(data).correct()\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "340a77c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: sme Correction: same\n",
      "ERROR: txt Correction: text\n",
      "ERROR: erors Correction: errors\n"
     ]
    }
   ],
   "source": [
    "# if you get errors, try \"brew install enchant\" \n",
    "# Don't have homebrew? Visit https://brew.sh/\n",
    "\n",
    "from enchant.checker import SpellChecker\n",
    "  \n",
    "# Creating the SpellChecker object\n",
    "chkr = SpellChecker(\"en_US\")\n",
    "  \n",
    "# Spelling error detection\n",
    "chkr.set_text(\"This is sme sample txt with erors.\")\n",
    "\n",
    "for err in chkr:\n",
    "    corrections = chkr.suggest(err.word)\n",
    "    if len(corrections) > 0:\n",
    "        # Get top likely correction\n",
    "        correction = corrections[0]\n",
    "        print(\"ERROR:\", err.word, \"Correction:\", correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e38121",
   "metadata": {},
   "source": [
    "# Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b03e593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'like', 'NLP', ',', '?']\n"
     ]
    }
   ],
   "source": [
    "# Get token from text using word tokenizers \n",
    "# described in the previous section\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "# Only need to run the below download once \n",
    "# nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "text = \"Hi I like NLP, do you?\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "stop_cleaned = [\n",
    "    w for w in tokens if w.lower() not in sw\n",
    "] \n",
    "# instead, you can also lowercase the text before tokenizing, \n",
    "# unless retaining case is required for your application\n",
    "\n",
    "print(stop_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42045e80",
   "metadata": {},
   "source": [
    "# Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4455ae7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural language processing\n"
     ]
    }
   ],
   "source": [
    "text = \"NATURAL LANGUAGE PROCESSING\"\n",
    "lower_cleaned = text.lower()\n",
    "print(lower_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "834f26a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car car fabric fabric comput comput\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "tokens = [\"cars\", \"car\", \"fabric\", \"fabrication\", \"computation\", \"computer\"]\n",
    "\n",
    "st = PorterStemmer()\n",
    "\n",
    "stemmed = \" \".join([st.stem(word) for word in tokens])\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8614d48c",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb53b7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fabric fabrication car car computation computer\n"
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "\n",
    "tokens = [\"fabric\", \"fabrication\", \"car\", \"cars\", \"computation\", \"computer\"]\n",
    "lemmatized = \" \".join(\n",
    "  [Word(word).lemmatize() for word in tokens]\n",
    ")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24a15224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bat see the cat\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(u'the bats saw the cats')\n",
    "\n",
    "# Lemmatize each token\n",
    "lemmatized = \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "print(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63ea2d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat bat computer compute\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/jsingh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokens = [\"cats\", \"bats\", \"computer\", \"compute\"]\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmatized = \" \".join(\n",
    "    [wnl.lemmatize(word) for word in tokens]\n",
    ")\n",
    "print(lemmatized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d78e6",
   "metadata": {},
   "source": [
    "# Example scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2dca967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'saw', 'big', 'snake', 'come', 'check', 'big', 'python', 'snake', 'video']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text = \"Hi all! I saw there was a big snake at https://xyz.he.com. Come check out the big python snake video!!!!\"\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "url_cleaned = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "cleaned = re.sub(\n",
    "    r\"[^a-zA-Z\\s+]+\", \" \", url_cleaned\n",
    ").lower()\n",
    "\n",
    "tokens = word_tokenize(cleaned)\n",
    "\n",
    "stop_removed = [\n",
    "    word\n",
    "    for word in tokens\n",
    "    if word not in stop_words\n",
    "]\n",
    "\n",
    "print(stop_removed)\n",
    "# >> ['hi', 'saw', 'big', 'snake', 'come', 'check', 'big', 'python', 'snake', 'video']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4041921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
